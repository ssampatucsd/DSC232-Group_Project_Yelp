{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cda430-cf33-43f8-bb17-8f60c892ce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-82va9ino because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "/usr/local/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcf7488-9257-4d41-9513-f01ec94739ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in /home/jtso1/.local/lib/python3.11/site-packages (5.22.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/jtso1/.local/lib/python3.11/site-packages (from plotly) (8.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from plotly) (23.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import site\n",
    "sys.path.append(site.getusersitepackages())\n",
    "!{sys.executable} -m pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79955556-9f6a-414c-a582-14bc9db86f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Yelp Data Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.executor.instances\", 8) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 32) \\\n",
    "    .config(\"spark.memory.fraction\", 0.6) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40c381b-091b-45c8-bb9f-bde63410f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = spark.read.json('yelp_academic_dataset_review.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e54760-4d96-44bc-bcbf-93c72de51d90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28459a7a-e133-4279-90ee-9e8d60762c17",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a37557-6ab0-4e46-a62c-e8227610d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count: 3358049\n",
      "+--------------------+-----+\n",
      "|                text|stars|\n",
      "+--------------------+-----+\n",
      "|If you decide to ...|  3.0|\n",
      "|I've taken a lot ...|  5.0|\n",
      "|Family diner. Had...|  3.0|\n",
      "|Wow!  Yummy, diff...|  5.0|\n",
      "|Cute interior and...|  4.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_df = review_df.select('text','stars')\n",
    "print('Total Count: ' + str(review_df.count()))\n",
    "review_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5a764-8717-4e16-baf4-a341ad30c632",
   "metadata": {},
   "source": [
    "### Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6b5d4d-28e7-4a6b-a87f-a0da1d036549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3358048"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows with any null values\n",
    "review_df = review_df.dropna()\n",
    "review_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca83fd-96c5-43e4-a197-442b09057cfd",
   "metadata": {},
   "source": [
    "### Add feature: 'review_length'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39785d47-d666-4cda-ad74-983e4a414ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+\n",
      "|                text|stars|review_length|\n",
      "+--------------------+-----+-------------+\n",
      "|If you decide to ...|  3.0|          513|\n",
      "|I've taken a lot ...|  5.0|          829|\n",
      "|Family diner. Had...|  3.0|          339|\n",
      "|Wow!  Yummy, diff...|  5.0|          243|\n",
      "|Cute interior and...|  4.0|          534|\n",
      "+--------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col, count, when, length\n",
    "\n",
    "# review_df = review_df.withColumn('review_length', length('text'))\n",
    "# review_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ede3af-d81f-4a1e-a1f4-84c27bf2e07a",
   "metadata": {},
   "source": [
    "### Tokenization and Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4821c19-c178-4263-82d7-f818e19484ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|                text|stars|               words|      filtered_words|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|If you decide to ...|  3.0|[if, you, decide,...|[decide, eat, her...|\n",
      "|I've taken a lot ...|  5.0|[i've, taken, a, ...|[taken, lot, spin...|\n",
      "|Family diner. Had...|  3.0|[family, diner., ...|[family, diner., ...|\n",
      "|Wow!  Yummy, diff...|  5.0|[wow!, , yummy,, ...|[wow!, , yummy,, ...|\n",
      "|Cute interior and...|  4.0|[cute, interior, ...|[cute, interior, ...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenized_df = tokenizer.transform(review_df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb78c9-6f42-4250-8394-70cf9e118e8e",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e7662a-0935-4511-9207-13d274c4d30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/jtso1/.local/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4684928-8b67-457b-95e0-20bbd7bf9eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+--------------------+--------------------+--------------------+\n",
      "|                text|stars|review_length|               words|      filtered_words|       stemmed_words|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------------+\n",
      "|If you decide to ...|  3.0|          513|[if, you, decide,...|[decide, eat, her...|[decid, eat, here...|\n",
      "|I've taken a lot ...|  5.0|          829|[i've, taken, a, ...|[taken, lot, spin...|[taken, lot, spin...|\n",
      "|Family diner. Had...|  3.0|          339|[family, diner., ...|[family, diner., ...|[famili, diner., ...|\n",
      "|Wow!  Yummy, diff...|  5.0|          243|[wow!, , yummy,, ...|[wow!, , yummy,, ...|[wow!, , yummy,, ...|\n",
      "|Cute interior and...|  4.0|          534|[cute, interior, ...|[cute, interior, ...|[cute, interior, ...|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# from pyspark.sql.functions import udf, col\n",
    "# from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "# stemmer=PorterStemmer()\n",
    "\n",
    "# def stem_words(words):\n",
    "#     return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# stem_udf = udf(stem_words, ArrayType(StringType()))\n",
    "\n",
    "# stemmed_df = filtered_df.withColumn(\"stemmed_words\", stem_udf(col(\"filtered_words\")))\n",
    "# stemmed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a193011-b0c9-4945-8ce6-97e7bdc28d58",
   "metadata": {},
   "source": [
    "### Sentiment Polarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8629c1df-76d3-45d1-8813-f01f517e8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#initialize the Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#define a function to calculate sentiment scores\n",
    "def sentiment_scores(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return float(scores['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f9c6ea-2990-4158-9a0d-fc773dcb7d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+------------------+\n",
      "|                text|stars|               words|      filtered_words|sentiment_polarity|\n",
      "+--------------------+-----+--------------------+--------------------+------------------+\n",
      "|If you decide to ...|  3.0|[if, you, decide,...|[decide, eat, her...|            0.8597|\n",
      "|I've taken a lot ...|  5.0|[i've, taken, a, ...|[taken, lot, spin...|            0.9858|\n",
      "|Family diner. Had...|  3.0|[family, diner., ...|[family, diner., ...|            0.9201|\n",
      "|Wow!  Yummy, diff...|  5.0|[wow!, , yummy,, ...|[wow!, , yummy,, ...|            0.9588|\n",
      "|Cute interior and...|  4.0|[cute, interior, ...|[cute, interior, ...|            0.9804|\n",
      "+--------------------+-----+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "sentiment_udf = udf(sentiment_scores, FloatType())\n",
    "\n",
    "polarity_df = filtered_df.withColumn('sentiment_polarity', sentiment_udf(col('text')))\n",
    "polarity_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85c091-cb02-4dbb-b08d-c0284296150b",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f0e118b-4bdf-4a6c-99b8-416959a8cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "|                text|stars|               words|      filtered_words|sentiment_polarity|          hashing_tf|              TF_IDF|\n",
      "+--------------------+-----+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "|If you decide to ...|  3.0|[if, you, decide,...|[decide, eat, her...|            0.8597|(262144,[12524,24...|(262144,[12524,24...|\n",
      "|I've taken a lot ...|  5.0|[i've, taken, a, ...|[taken, lot, spin...|            0.9858|(262144,[18176,29...|(262144,[18176,29...|\n",
      "|Family diner. Had...|  3.0|[family, diner., ...|[family, diner., ...|            0.9201|(262144,[578,1261...|(262144,[578,1261...|\n",
      "|Wow!  Yummy, diff...|  5.0|[wow!, , yummy,, ...|[wow!, , yummy,, ...|            0.9588|(262144,[30899,45...|(262144,[30899,45...|\n",
      "|Cute interior and...|  4.0|[cute, interior, ...|[cute, interior, ...|            0.9804|(262144,[1689,495...|(262144,[1689,495...|\n",
      "+--------------------+-----+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Apply TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"hashing_tf\")\n",
    "tf_df = hashing_tf.transform(polarity_df)\n",
    "idf = IDF(inputCol=\"hashing_tf\", outputCol=\"TF_IDF\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n",
    "\n",
    "tfidf_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ecd44-68ae-45d9-b535-e8e8f1b75db9",
   "metadata": {},
   "source": [
    "### Prepare data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9406844d-72e2-494d-8944-123bded53619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31bcb201-4fa0-4d6f-877a-ebe30d3fed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembling features\n",
    "assembler = VectorAssembler(inputCols=['sentiment_polarity', 'TF_IDF'], outputCol='combined_features')\n",
    "final_df = assembler.transform(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c6d6c-7870-4fb5-a6f5-b6a3a92452df",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff3cecd3-4977-4cd0-93fc-d0424c613f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   combined_features|stars|\n",
      "+--------------------+-----+\n",
      "|(262145,[0,12525,...|  3.0|\n",
      "|(262145,[0,18177,...|  5.0|\n",
      "|(262145,[0,579,12...|  3.0|\n",
      "|(262145,[0,30900,...|  5.0|\n",
      "|(262145,[0,1690,4...|  4.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#select features\n",
    "select_df = final_df.select('combined_features', 'stars')\n",
    "select_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21590a65-bee9-4f71-b57f-cdb422b67618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# X = select_df['combined_features']\n",
    "# y = select_df['stars']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c4ea6-276c-4cb8-9d5c-de1258eb0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# accuracy = model.score(X_test, y_test)\n",
    "# print(f'Accuracy: {accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccf2e5f0-80e6-48a5-b3b5-c6caf0b2d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#split data into training and test sets\n",
    "train_df, test_df = select_df.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "#define logistic regression model\n",
    "lr = LogisticRegression(featuresCol='combined_features', labelCol='stars', family=\"multinomial\")\n",
    "\n",
    "#train the model\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# #make predictions\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5833483-338d-4ead-96ca-86553c49bbd9",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9feec8ff-175c-4c4b-84ab-114d03bbcd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.6257126027989651\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='stars', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b0c12-e225-465c-8ddd-0ea09a9baf1d",
   "metadata": {},
   "source": [
    "### Overfitting/Underfitting Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a2dc94e-a851-4613-90cc-43ae2599359c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.pandas' has no attribute 'scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m df_fitting_graph \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m,error]], columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m new_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(df_fitting_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(df_fitting_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[0;32m---> 18\u001b[0m df_fitting_graph \u001b[38;5;241m=\u001b[39m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m(df_fitting_graph, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting Graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m df_fitting_graph\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/__init__.py:165\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(MissingPandasLikeGeneralFunctions, key)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyspark.pandas\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (key))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.pandas' has no attribute 'scatter'"
     ]
    }
   ],
   "source": [
    "error = 1 - accuracy\n",
    "#model_complexity = []\n",
    "#model_complexity.append('Logistic Regression')\n",
    "#input_rows = model_complexity\n",
    "\n",
    "#rows_list = []\n",
    "#for row in input_rows:\n",
    "    #dict1 = {}\n",
    "    #dict1.update({row:error}) \n",
    "    #rows_list.append(dict1)\n",
    "\n",
    "#df_fitting_graph = px.scatter(rows_list, labels={'index':'count'})\n",
    "#df_fitting_graph.show()\n",
    "\n",
    "df_fitting_graph = pd.DataFrame([['Logistic Regression',error]], columns= ['Model','loss'])\n",
    "new_dict = {\"Model\": list(df_fitting_graph['Model']), \"error\": list(df_fitting_graph['loss'])}\n",
    "\n",
    "df_fitting_graph = ps.scatter(df_fitting_graph, x='Model',y='loss',title='Fitting Graph')\n",
    "df_fitting_graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854692db-4c2b-4a9c-82f0-cc729dae0012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dde45-296f-4e8a-85fe-991b6ec923ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47a4ea-7a2b-4296-8f1e-0ad1c5614a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
